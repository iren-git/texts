{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = False\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3995803, 13) 0.0688212106553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000025</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Монтаж кровли</td>\n",
       "      <td>Выполняем  монтаж кровли фальцевой ^p Тел:8@@P...</td>\n",
       "      <td>{\"Вид услуги\":\"Ремонт, строительство\"}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000101</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Ford Focus, 2011</td>\n",
       "      <td>Автомобиль в отличном техническом состоянии, в...</td>\n",
       "      <td>{\"Марка\":\"Ford\", \"Модель\":\"Focus\", \"Год выпуск...</td>\n",
       "      <td>365000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000132</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>Турбина 3.0 Bar</td>\n",
       "      <td>Продам турбину на двигатель V-6 . V-8 и мощнее...</td>\n",
       "      <td>{\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid     category                subcategory              title  \\\n",
       "0  10000010    Транспорт      Автомобили с пробегом  Toyota Sera, 1991   \n",
       "1  10000025       Услуги          Предложения услуг      Монтаж кровли   \n",
       "2  10000094  Личные вещи  Одежда, обувь, аксессуары   Костюм Steilmann   \n",
       "3  10000101    Транспорт      Автомобили с пробегом   Ford Focus, 2011   \n",
       "4  10000132    Транспорт      Запчасти и аксессуары    Турбина 3.0 Bar   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Выполняем  монтаж кровли фальцевой ^p Тел:8@@P...   \n",
       "2  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "3  Автомобиль в отличном техническом состоянии, в...   \n",
       "4  Продам турбину на двигатель V-6 . V-8 и мощнее...   \n",
       "\n",
       "                                               attrs   price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000        NaN   \n",
       "1             {\"Вид услуги\":\"Ремонт, строительство\"}       0        NaN   \n",
       "2  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...    1500        NaN   \n",
       "3  {\"Марка\":\"Ford\", \"Модель\":\"Focus\", \"Год выпуск...  365000        NaN   \n",
       "4  {\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...    5000        NaN   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           1           0         0        22.38  \n",
       "2           0           0           0         0         0.41  \n",
       "3           0           0           0         0         8.87  \n",
       "4           0           0           0         0        11.82  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.0688212106553\n",
      "Count: 3995803\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.49999636361\n",
      "Count: 549996\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "indices = np.concatenate((np.random.permutation(np.arange(len(old_df))[np.array(old_df.is_blocked) == 0])[:275000],\n",
    "                         np.arange(len(old_df))[np.array(old_df.is_blocked) == 1]))\n",
    "df = old_df.iloc[indices]\n",
    "\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "#from stemming.porter2 import stem\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "mapping = {}\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "#        if token not in mapping.keys():\n",
    "#            mapping[token] = stem(token)\n",
    "#        token_counts[mapping[token]] +=1\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63f9aee6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = {x:y for x, y in token_counts.items() if y > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 83052\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "#        token_ids = map(lambda token: token_to_id.get(mapping[token], 0), tokens)[:max_len]\n",
    "        token_ids = map(lambda token: token_to_id.get(token, 0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549996, 15)\n",
      "Ford Mondeo, 1993 -> [13124 27399  5370     0     0     0     0     0     0     0] ...\n",
      "Дом 124 м² на участке 4.5 сот. -> [29837 17148 26094 28003 46207   103 80410  4892     0     0] ...\n",
      "Индюшата, перепелята -> [0 0 0 0 0 0 0 0 0 0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":category_name, \"subcategory\":subcategory_name} for category_name, subcategory_name in \n",
    "             data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549996,)\n",
      "(549996, 15)\n",
      "(549996, 150)\n",
      "(549996, 67)\n"
     ]
    }
   ],
   "source": [
    "print target.shape\n",
    "print title_tokens.shape\n",
    "print desc_tokens.shape\n",
    "print df_non_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, desc_tokens, df_non_text, target, \n",
    "                                                                                               test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "    \n",
    "    data_tuple = (title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts)\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikhism/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "from lasagne import layers, updates, nonlinearities\n",
    "from lasagne.objectives import binary_crossentropy\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = theano.tensor.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = theano.tensor.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = theano.tensor.matrix(\"categories\",dtype='float32')\n",
    "target_y = theano.tensor.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id) + 1, output_size=128)\n",
    "descr_nn = layers.LSTMLayer(descr_nn, 20)\n",
    "descr_nn = layers.SliceLayer(descr_nn, indices=-1, axis=1)\n",
    "\n",
    "# Titles\n",
    "title_nn = layers.EmbeddingLayer(title_inp, input_size=len(title_tr) + 1, output_size=128)\n",
    "title_nn = layers.Conv1DLayer(title_nn, num_filters=40, filter_size=6)\n",
    "title_nn = layers.MaxPool1DLayer(title_nn, pool_size=2)\n",
    "title_nn = layers.DenseLayer(title_nn, num_units=64)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = layers.DenseLayer(cat_inp, num_units=128)\n",
    "cat_nn = layers.DenseLayer(cat_inp, num_units=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = layers.ConcatLayer([descr_nn, title_nn, cat_nn])                             \n",
    "\n",
    "nn = layers.DenseLayer(nn,32)\n",
    "nn = layers.DropoutLayer(nn,p=0.5)\n",
    "nn = layers.DenseLayer(nn,1,nonlinearity=nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "#Did'n manage to import it. Using binary cross entropy instead\n",
    "loss = binary_crossentropy(prediction, target_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = updates.adam(theano.grad(loss, weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = layers.get_output(nn, deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = binary_crossentropy(det_prediction, target_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "#     if shuffle:\n",
    "    indices = np.arange(len(arrays[0]))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "#         if shuffle:\n",
    "        excerpt = list(indices[start_idx:start_idx + batchsize])\n",
    "#         else:\n",
    "#             excerpt = slice(start_idx, start_idx + batchsize)\n",
    "\n",
    "    \n",
    "        yield [arr.take(excerpt, axis=0) for arr in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr.take(excerpt, axis=0) for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((494996, 150), (494996, 15), (494996, 67), (494996,))\n"
     ]
    }
   ],
   "source": [
    "print(desc_tr.shape, title_tr.shape, nontext_tr.shape, target_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: nan\n",
      "\tacc: 0.383663366337\n",
      "\tauc: 0.28241746777\n",
      "\tap@k: 0.108100688637\n",
      "Val:\n",
      "\tloss: nan\n",
      "\tacc: 0.496831683168\n",
      "\tauc: 0.499078766404\n",
      "\tap@k: 0.496518630657\n",
      "Train:\n",
      "\tloss: nan\n",
      "\tacc: 0.349603960396\n",
      "\tauc: 0.281218469853\n",
      "\tap@k: 0.183666879894\n",
      "Val:\n",
      "\tloss: nan\n",
      "\tacc: 0.503861386139\n",
      "\tauc: 0.492198049377\n",
      "\tap@k: 0.457526404419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "#     print len(desc_tr), len(title_tr), len(nontext_tr), len(target_tr)\n",
    "    for tup in enumerate(iterate_minibatches(desc_tr, title_tr, nontext_tr, target_tr, batchsize=batch_size, \n",
    "                                                                        shuffle=True)):\n",
    "        j = tup[0]\n",
    "        b_desc,b_title,b_cat, b_y = tup[1]\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss, pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss, pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
